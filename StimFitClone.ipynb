{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "%matplotlib osx\n",
    "import stf\n",
    "import eventanalysis\n",
    "import utilities\n",
    "import matplotlib.pyplot as plt\n",
    "import minievents\n",
    "import numpy as np\n",
    "from scipy import stats, interpolate, optimize, integrate\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'eventanalysis' from 'eventanalysis.pyc'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload when edits happen\n",
    "reload(utilities)\n",
    "reload(stf)\n",
    "reload(eventanalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified events\n",
      "Merged events\n",
      "Quantified events\n",
      "Showed big events\n",
      "Obtained Fourier amplitudes\n",
      "Assessed significance of Fourier amplitudes\n",
      "Grouped spikes together\n",
      "Processed spike groups\n",
      "Removed spikes\n"
     ]
    }
   ],
   "source": [
    "# Analyse big events and remove noise\n",
    "reload(eventanalysis)\n",
    "EA = eventanalysis.EventAnalysis()\n",
    "EA.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified background noise\n",
      "8.14209008217\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Analyse minievents\n",
    "reload(minievents)\n",
    "MEH = minievents.MiniEventHandler(EA.data[-1],mask=EA.mask,event_number_barrier=10000,bayes_bound=0.0,score_bound=1.5)\n",
    "MEH.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19758\n",
    "MEH.event_box.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11aa7cd90>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.loglog(MEH.event_box['rise'],MEH.event_box['decay'],ls='none',marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/douglasboubert/anaconda3/envs/py27/lib/python2.7/site-packages/matplotlib/axes/_base.py:2961: UserWarning: Attempting to set identical left==right results\n",
      "in singular transformations; automatically expanding.\n",
      "left=1e-10, right=1e-10\n",
      "  'left=%s, right=%s') % (left, right))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "X = np.stack([np.log10(MEH.event_box['rise']),np.log10(MEH.event_box['decay'])]).T\n",
    "n_clusters = 4\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n",
    "kmeans.labels_\n",
    "for i in range(n_clusters):\n",
    "    inclust = np.where(kmeans.labels_==i)\n",
    "    plt.loglog(MEH.event_box['rise'][inclust],MEH.event_box['decay'][inclust],ls='none',marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all the events together\n",
    "e_time = np.linspace(0.,10.,1000)\n",
    "colorlist= ['deepskyblue','orangered','forestgreen','crimson']\n",
    "for i in range(MEH.event_box['N']):\n",
    "    plt.plot(e_time,(MEH.event_box['scale'][i])*MEH._biexponential(e_time,RISE=MEH.event_box['rise'][i],DECAY=MEH.event_box['decay'][i]),c=colorlist[kmeans.labels_[i]],alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x121932810>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.loglog(MEH.event_box['peak_time'],MEH.event_box['rise'],ls='none',marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(MEH.event_box['rchi2'].size):\n",
    "    if MEH.event_box['rchi2'][i]>1.2:\n",
    "        print MEH.event_box['rchi2'][i], int(MEH.event_box['t'][i]/MEH.dt), MEH.event_box['t'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bins = np.arange(-1,10,0.2)\n",
    "plt.hist(MEH.event_box['bayes_factor'],bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1219ebbd0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MEH._post_process_event_box(significance_threshold=2.0,charge_threshold=5.0,rchi2_threshold=1.0)\n",
    "plt.plot(MEH.t,MEH.data)\n",
    "plt.plot(MEH.t,MEH.data_residual)\n",
    "plt.plot(MEH.t,MEH.score)\n",
    "plt.scatter(MEH.raw_event_box['t'],np.zeros(MEH.raw_event_box['t'].size),zorder=10,c='purple')\n",
    "plt.scatter(MEH.event_box['t'],np.zeros(MEH.event_box['t'].size),zorder=10)\n",
    "#plt.xlim([17355,17365])\n",
    "#plt.ylim([-20,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MEH.event_box['rchi2'][340]\n",
    "MEH.data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins= np.arange(0,100,1)\n",
    "plt.hexbin(MEH.event_box['area'],-MEH.event_box['scale']/MEH.event_box['noise'],mincnt=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEH._post_process_event_box()\n",
    "plt.plot(MEH.t,MEH.data)\n",
    "plt.plot(MEH.t,MEH.data_residual)\n",
    "plt.plot(MEH.t,MEH.score)\n",
    "plt.scatter(MEH.raw_event_box['t'],np.zeros(MEH.raw_event_box['t'].size),zorder=10,c='purple')\n",
    "colorlist= ['deepskyblue','orangered','forestgreen','crimson']\n",
    "for ic in range(n_clusters):\n",
    "    inclust = np.where(kmeans.labels_==ic)\n",
    "    plt.scatter(MEH.event_box['t'][inclust],np.zeros(MEH.event_box['t'][inclust].size),zorder=10,c=colorlist[ic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_score = -np.fft.irfft(MEH.ft_data * MEH.ft_template.conjugate(),n=MEH.data.size)\n",
    "#_SCORE = (_score-MEH.score_med)/MEH.score_std\n",
    "_SCORE = (_score-left_score_med)/left_score_std\n",
    "sc_bins = np.arange(start=_SCORE.min()-1.0,stop=_SCORE.max()+1.0,step=0.1)\n",
    "sc_midbins = (sc_bins[1:]+sc_bins[:-1])/2.0\n",
    "lnkde = stats.gaussian_kde(_SCORE).logpdf(sc_midbins)\n",
    "sc_lnkde_interp = interpolate.interp1d(sc_midbins,lnkde)\n",
    "sc_lnev = 2.0*(sc_lnkde_interp(_SCORE)-stats.norm.logpdf(_SCORE)+MEH.log_bayes_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(-10,10,0.01)\n",
    "plt.hist(sc_lnev,bins)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "plt.hexbin(sc_lnev,_SCORE,mincnt=1,norm=LogNorm(),gridsize=200,extent=[-10,10,-5,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(18413/MEH.dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = np.where((_SCORE>1.5))\n",
    "plt.plot(MEH.t,MEH.data)\n",
    "plt.scatter(MEH.t[valid],np.zeros(valid[0].size),zorder=10,c='orangered')\n",
    "plt.plot(MEH.t,_SCORE)\n",
    "plt.plot(MEH.t,_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sc_midbins,lnkde)\n",
    "plt.plot(sc_midbins,stats.norm.logpdf(sc_midbins,scale=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(_SCORE,100)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_score_med, all_score_std = utilities.rolling(_score,FUNC='SUFFICIENT_STATISTICS',WINDOW=int(0.1*MEH.median_window))\n",
    "left_score_med, left_score_std = utilities.rolling(_score,FUNC='SUFFICIENT_STATISTICS_LEFT',WINDOW=int(0.1*MEH.median_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_LEFT_SCORE = (_score-left_score_med)/left_score_std\n",
    "_ALL_SCORE = (_score-all_score_med)/all_score_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-10,20,100)\n",
    "plt.hist(_LEFT_SCORE,bins,alpha=0.5)\n",
    "plt.hist(_ALL_SCORE,bins,alpha=0.5)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt at doing two events simultaneously\n",
    "from scipy import optimize\n",
    "from scipy import signal\n",
    "# Pull out event and play with fitting\n",
    "_npoints = int(10.0/MEH.dt)\n",
    "#_idx = 19758\n",
    "#_idx = 83065\n",
    "#_idx=86431\n",
    "#_idx = 12108\n",
    "#_idx = 144216\n",
    "#_idx = 92527\n",
    "#_idx = 52164\n",
    "#_idx = int(17225.0/MEH.dt)\n",
    "_idx = int(13280.0/MEH.dt)\n",
    "NOISE = MEH.noise_std[_idx]\n",
    "MEDIAN = MEH.noise_med[_idx]\n",
    "\n",
    "\n",
    "# Are there multiple significant peaks?\n",
    "loop_bool = True\n",
    "_peaks = [_npoints]\n",
    "\n",
    "_previous_peaks = []\n",
    "_first_peak = _idx+min(_peaks)-_npoints\n",
    "_last_peak = _idx+max(_peaks)-_npoints\n",
    "_peak_gap = 2\n",
    "_backwards_idx = 5\n",
    "_left_extension = _npoints\n",
    "_right_extension = int(1.5*_npoints)\n",
    "_default_peak_time = MEH._biexponential_peak_time(RISE=MEH.default_rise,DECAY=MEH.default_decay)\n",
    "while loop_bool:\n",
    "    _score = MEH.score_initial[_first_peak-_left_extension:_last_peak+_right_extension]\n",
    "    _data = MEH.data[_first_peak-_left_extension:_last_peak+_right_extension]\n",
    "    _time = MEH.t[_first_peak-_left_extension:_last_peak+_right_extension]\n",
    "    _peak_gap = int(1.0/MEH.dt)\n",
    "    _peaks = [] # we start with the original peak\n",
    "    _peaks_heights = []\n",
    "    _suggested_peaks, _peaks_props = signal.find_peaks(-_data,prominence=NOISE*2,height = 5.0,width=2)\n",
    "    #_suggested_peaks = signal.find_peaks_cwt(_score, np.arange(5,10))\n",
    "    print(_suggested_peaks)\n",
    "    for _peak_idx,_peak_height in zip(_suggested_peaks,_peaks_props['peak_heights']):\n",
    "        _mad_idx = int((_time[_peak_idx]-_default_peak_time)/MEH.dt)\n",
    "        _max_score = MEH.score_initial[_mad_idx]\n",
    "        if _max_score>0.5:\n",
    "            _peaks.append(_peak_idx)\n",
    "            _peaks_heights.append(-_peak_height)\n",
    "    print _peaks\n",
    "    \n",
    "    if _peaks == _previous_peaks:\n",
    "        loop_bool=False\n",
    "    _n_peaks = len(_peaks)\n",
    "    print len(_peaks), _n_peaks\n",
    "    _first_peak = _first_peak+min(_peaks)-_npoints\n",
    "    _last_peak = _first_peak+max(_peaks)-_npoints\n",
    "    _previous_peaks = _peaks\n",
    "\n",
    "# Pull out time and data information\n",
    "#_first_peak = _idx+min(_peaks)-_npoints\n",
    "#_last_peak = _idx+max(_peaks)-_npoints\n",
    "_time = MEH.t[_first_peak-_left_extension:_last_peak+_right_extension]\n",
    "_data = MEH.data[_first_peak-_left_extension:_last_peak+_right_extension]\n",
    "_score = MEH.score_initial[_first_peak-_left_extension:_last_peak+_right_extension]\n",
    "_PEAKS = [_p for _p in _peaks]\n",
    "\n",
    "# Gather useful quantities\n",
    "_data_scale = np.abs(_data.max()-_data.min())\n",
    "_range_start_t = [3.*MEH.default_rise,1.*MEH.default_rise]\n",
    "\n",
    "_weights= np.exp(-((_time[:,np.newaxis]-_time[_peaks])**2.0/(2.0*(3.0*MEH.default_decay)**2.0))).sum(axis=1)\n",
    "\n",
    "def _mod_template(T,X):\n",
    "    SCALE, RISE, DECAY, START_T = X\n",
    "    return SCALE * MEH._biexponential(T-START_T,RISE=RISE,DECAY=DECAY)\n",
    "\n",
    "def _super_model(T,X,_NMODEL):\n",
    "    OFFSET, GRADIENT, EVENT_PARAMS = X[0], X[1], X[2:].reshape((_NMODEL,4))\n",
    "    _model = OFFSET + GRADIENT*(T-T[_npoints])\n",
    "    for _model_index in range(_NMODEL):\n",
    "        _model += _mod_template(T,EVENT_PARAMS[_model_index])\n",
    "    return _model\n",
    "\n",
    "def _sub_model(T,X,_NMODEL,_nMODEL):\n",
    "    OFFSET, GRADIENT, EVENT_PARAMS = X[0], X[1], X[2:].reshape((_NMODEL,4))\n",
    "    _model = OFFSET + GRADIENT*(T-T[_npoints]) + _mod_template(T,EVENT_PARAMS[_nMODEL])\n",
    "    return _model\n",
    "\n",
    "_gauss_norm = -0.5*np.log(2.0*np.pi*NOISE**2.0)\n",
    "def _target(X,_NMODEL=1):\n",
    "    _model = _super_model(_time,X,_NMODEL)\n",
    "    return (-(-0.5*(_data-_model)**2.0/NOISE**2.0+_gauss_norm)*_weights).sum()+1.5*np.log(1.0+X[1]**2.0)\n",
    "\n",
    "# Set up initial location and bounds\n",
    "_X0 = [MEDIAN,0.0]\n",
    "#_delta = abs((_data[-1]-_data[0])/(_time[-1]-_time[0]))\n",
    "_bnds = [(_data.min(), _data.max()),(-1.0,1.0)]\n",
    "for _p,_h in zip(_PEAKS,_peaks_heights):\n",
    "    _mean_start_t = _time[_p]-_default_peak_time-MEH.default_rise\n",
    "    _X0 += [_h, MEH.default_rise, MEH.default_decay,_mean_start_t]\n",
    "    print (_h*0.5, _h*1.5)\n",
    "    _bnds += [(_h*1.5, _h*0.5), (1e-10, 1e5), (1e-2, 1e2), (_mean_start_t-_range_start_t[0], _mean_start_t+_range_start_t[1])]\n",
    "_opt = {'gtol':1e-10,'ftol':1e-10,'maxfun':250000}\n",
    "_res = optimize.minimize(_target,_X0,method='L-BFGS-B', tol=1e-10, bounds=_bnds, options=_opt, args=(len(_peaks)))\n",
    "RCHI2 = np.mean(np.abs(_data-_super_model(_time,_res['x'],len(_peaks)))/NOISE)\n",
    "#OFFSET, SCALE, RISE, DECAY, START_T = _res['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(_time,_data)\n",
    "plt.plot(_time,_score)\n",
    "plt.plot(_time,_super_model(_time,_res['x'],len(_peaks)))\n",
    "#plt.plot(_time,_data-_super_model(_time,_res['x'],len(_peaks)))\n",
    "plt.scatter(_time[_peaks],np.zeros(len(_peaks)),zorder=10,c='orangered')\n",
    "#plt.scatter(_res['x'][5::4],np.ones(len(_peaks)),zorder=10,c='purple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "plt.plot(_time,_data)\n",
    "plt.plot(_time,_score)\n",
    "tmppeaks,props = signal.find_peaks(-_data,prominence=NOISE*2,height = 5.0,width=2)\n",
    "truepeaks = np.array([i for i in tmppeaks if _score[i-5:i].max()>0.5])\n",
    "plt.scatter(_time[tmppeaks],np.zeros(tmppeaks.size),zorder=10,c='purple')\n",
    "plt.scatter(_time[truepeaks],np.zeros(truepeaks.size),zorder=10,c='orangered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(_time,_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(17200/MEH.dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(MEH.t[_idx-_npoints:_idx+_npoints*2],MEH.data[_idx-_npoints:_idx+_npoints*2])\n",
    "plt.plot(MEH.t[_idx-_npoints:_idx+_npoints*2],MEH.data_residual[_idx-_npoints:_idx+_npoints*2])\n",
    "plt.plot(MEH.t[_idx-_npoints:_idx+_npoints*2],MEH.score[_idx-_npoints:_idx+_npoints*2])\n",
    "\n",
    "#plt.scatter(MEH.t[_idx-_npoints:_idx+_npoints*2][highpeaks],np.zeros(highpeaks.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "plt.plot(MEH.t,MEH.data)\n",
    "plt.plot(MEH.t,MEH.score)\n",
    "tmppeaks,props = signal.find_peaks(-MEH.data,prominence=MEH.noise_std*2,height = 5.0,width=2,wlen=20)\n",
    "truepeaks = np.array([i for i in tmppeaks if MEH.score[i-5:i].max()>3.0])\n",
    "plt.scatter(MEH.t[truepeaks],np.zeros(truepeaks.size),zorder=10,c='orangered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(16530.0/MEH.dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = EA.data[-1]\n",
    "import pandas as pd\n",
    "def rolling(DATA,WINDOW=50,FUNC='MEDIAN',EDGE_METHOD='SHIFTING'):\n",
    "    def cleaning(RAW_ROLLING):\n",
    "        RAW_ROLLING[:WINDOW] = np.roll(RAW_ROLLING,-WINDOW)[:WINDOW]\n",
    "        RAW_ROLLING[-WINDOW:] = np.roll(RAW_ROLLING,WINDOW)[-WINDOW:]\n",
    "        return RAW_ROLLING\n",
    "    \n",
    "    if FUNC=='MEDIAN':\n",
    "        raw_rolling = pd.DataFrame(DATA).rolling(window=WINDOW,center=True).median().values.flatten()\n",
    "    elif FUNC=='MEAN':\n",
    "        raw_rolling = pd.DataFrame(DATA).rolling(window=WINDOW,center=True).mean().values.flatten()\n",
    "    elif FUNC=='STD':\n",
    "        raw_rolling = pd.DataFrame(DATA).rolling(window=WINDOW,center=True).std().values.flatten()\n",
    "    elif FUNC=='SUM':\n",
    "        raw_rolling = pd.DataFrame(DATA).rolling(window=WINDOW,center=True).sum().values.flatten()\n",
    "    elif FUNC=='SMOOTH_MEDIAN':\n",
    "        raw_rolling_q1 = pd.DataFrame(DATA).rolling(window=WINDOW,center=True).quantile(0.40).values.flatten()\n",
    "        raw_rolling_q3 = pd.DataFrame(DATA).rolling(window=WINDOW,center=True).quantile(0.60).values.flatten()\n",
    "        raw_rolling = np.median(np.stack([raw_rolling_q1,raw_rolling_q3]),axis=0)\n",
    "    elif FUNC=='SUFFICIENT_STATISTICS':\n",
    "        raw_rolling_q1 = pd.DataFrame(DATA).rolling(window=WINDOW,center=True).quantile(0.25).values.flatten()\n",
    "        raw_rolling_q2 = pd.DataFrame(DATA).rolling(window=WINDOW,center=True).quantile(0.50).values.flatten()\n",
    "        raw_rolling_q3 = pd.DataFrame(DATA).rolling(window=WINDOW,center=True).quantile(0.75).values.flatten()\n",
    "        raw_rolling_std = (raw_rolling_q3-raw_rolling_q1)/1.349\n",
    "        return cleaning(raw_rolling_q2),cleaning(raw_rolling_std)\n",
    "        \n",
    "    return cleaning(raw_rolling)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med, std =rolling(data,FUNC='SUFFICIENT_STATISTICS',WINDOW=MEH.median_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _triexponential(T,RISE1=1.0,RISE2=2.0,DECAY=3.0,F=0.5):\n",
    "    NORM = 1.0#1/((DECAY/(RISE+DECAY))*(RISE/(RISE+DECAY))**(RISE/DECAY))\n",
    "    retarray = np.zeros(T.shape[0])\n",
    "    postime = T>-10.\n",
    "    retarray[postime] = NORM*(1.0-F*np.exp(-T[postime]/RISE1)-(1.0-F)*np.exp(-T[postime]/RISE2))*(np.exp(-T[postime]/DECAY))\n",
    "    return retarray#/retarray.max()\n",
    "\n",
    "rise1 = 2.0\n",
    "rise2 = 10.0\n",
    "decay = 5.0\n",
    "T = np.linspace(0.,10.,10000)\n",
    "#frange = np.linspace(-rise1/(rise2-rise1),1.0,100)\n",
    "frange = np.linspace(0.0,1.0,100)\n",
    "for f in frange:\n",
    "    plt.plot(T,_triexponential(T,RISE1=rise1,RISE2=rise2,DECAY=decay,F=f),c=plt.cm.viridis((f-frange.min())/(frange.max()-frange.min())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_x = scipy.optimize.fmin(lambda x: -f(x), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _triexponential(T,RISE1=1.0,RISE2=2.0,DECAY=3.0,F=0.5):\n",
    "    NORM = 1.0#1/((DECAY/(RISE+DECAY))*(RISE/(RISE+DECAY))**(RISE/DECAY))\n",
    "    retarray = np.zeros(T.shape[0])\n",
    "    retarray = NORM*(1.0-F*np.exp(-T/RISE1)-(1.0-F)*np.exp(-T/RISE2))*(np.exp(-T/DECAY))\n",
    "    return retarray#/retarray.max()\n",
    "\n",
    "rise1 = 1.0\n",
    "rise2 = 2.0\n",
    "decay = 5.0\n",
    "f = 0.5\n",
    "T = np.linspace(0.,10.,10000)\n",
    "plt.plot(T,_triexponential(T,RISE1=rise1,RISE2=rise2,DECAY=decay,F=f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use minimize_scalar\n",
    "from math import exp\n",
    "rise1 = 1.0\n",
    "rise2 = 2.0\n",
    "decay = 3.0\n",
    "f = 0.5\n",
    "T = np.linspace(0.,10.,10000)\n",
    "def _triexp_scalar(x):\n",
    "    return -(1.0-f*exp(-x/rise1)-(1.0-f)*exp(-x/rise2))*exp(-x/decay)\n",
    "_res = optimize.minimize_scalar(_triexp_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "rise1 = 1.0\n",
    "rise2 = 2.0\n",
    "decay = 3.0\n",
    "f = 0.5\n",
    "T = np.linspace(0.,10.,10000)\n",
    "def _triexp_scalar(x):\n",
    "    return -(1.0-f*exp(-x/rise1)-(1.0-f)*exp(-x/rise2))*exp(-x/decay)\n",
    "_res = optimize.minimize_scalar(_triexp_scalar,method='golden',tol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit optimize.minimize_scalar(_triexp_scalar,method='brent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit optimize.minimize_scalar(_triexp_scalar,method='golden',tol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit optimize.minimize_scalar(_triexp_scalar,method='bounded',bounds=(0,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "def _triexponential_peak(RISE,ETA,DECAY,F):\n",
    "    RISE1 = RISE\n",
    "    RISE2 = RISE*ETA\n",
    "    G0_A = F*(1.0+DECAY/RISE1)\n",
    "    G0_B = (1.0-F)*(1.0+DECAY/RISE2)\n",
    "    G1_A = -G0_A/RISE1\n",
    "    G1_B = -G0_B/RISE2\n",
    "    G2_A = -G1_A/RISE1\n",
    "    G2_B = -G1_B/RISE2\n",
    "    def _G0(x):\n",
    "        return -1.0+G0_A*exp(-x/RISE1)+G0_B*exp(-x/RISE2)\n",
    "    def _G1(x):\n",
    "        return G1_A*exp(-x/RISE1)+G1_B*exp(-x/RISE2)\n",
    "    def _G2(x):\n",
    "        return G2_A*exp(-x/RISE1)+G2_B*exp(-x/RISE2)\n",
    "\n",
    "    root = optimize.newton(_G0, 1.0, fprime=_G1,fprime2=_G2)\n",
    "    return root\n",
    "\n",
    "def _triexponential(T,RISE=1.0,ETA=1.0,DECAY=1.0,F=0.5,NORMED=True):\n",
    "    RISE1 = RISE\n",
    "    RISE2 = RISE*ETA\n",
    "    retarray = np.zeros(T.shape[0])\n",
    "    retarray = (1.0-F*np.exp(-T/RISE1)-(1.0-F)*np.exp(-T/RISE2))*(np.exp(-T/DECAY))\n",
    "    if NORMED == True:\n",
    "        PEAK = _triexponential_peak(RISE,ETA,DECAY,F)\n",
    "        NORM = _triexponential(np.array([PEAK]),RISE,ETA,DECAY,F,NORMED=False)[0]\n",
    "        retarray /= NORM\n",
    "    return retarray\n",
    "\n",
    "def _triexponential_area(RISE=1.0,ETA=1.0,DECAY=1.0,F=0.5,NORMED=True):\n",
    "    RISE1 = RISE\n",
    "    RISE2 = RISE*ETA\n",
    "    AREA = ((DECAY+(1.0-F)*RISE1+F*RISE2)/((1.0+RISE1/DECAY)*(1.0+RISE2/DECAY)))\n",
    "    if NORMED == True:\n",
    "        PEAK = _triexponential_peak(RISE,ETA,DECAY,F)\n",
    "        NORM = _triexponential(np.array([PEAK]),RISE,ETA,DECAY,F,NORMED=False)[0]\n",
    "        AREA /= NORM\n",
    "    return AREA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rise = 1.0\n",
    "eta = 2.0\n",
    "decay = 5.0\n",
    "f = 0.5\n",
    "T = np.linspace(0.,10.,10000)\n",
    "#varrange = np.logspace(0.0,5,100)\n",
    "varrange = np.linspace(0.,1.0,100)\n",
    "\n",
    "for v in varrange:\n",
    "    plt.plot(T,_triexponential(T,RISE=rise,ETA=eta,DECAY=decay,F=v),c=plt.cm.viridis((v)/(1.0)))\n",
    "plt.plot(T,_triexponential(T,RISE=rise,ETA=eta,DECAY=decay,F=0.5),c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rise = 1.0\n",
    "eta = 2.0\n",
    "decay = 5.0\n",
    "f = 0.5\n",
    "T = np.linspace(0.,10.,10000)\n",
    "varrange = np.logspace(0.0,10,100)\n",
    "\n",
    "for v in varrange:\n",
    "    plt.plot(T,_triexponential(T,RISE=rise,ETA=v,DECAY=decay,F=f),c=plt.cm.viridis(np.log10(v)/(5.0)))\n",
    "plt.plot(T,_triexponential(T,RISE=rise,ETA=eta,DECAY=decay,F=0.5),c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log10(varrange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrand(t):\n",
    "    return _triexponential(np.array([t]),RISE=1.0,ETA=2.0,DECAY=5.0,F=0.8,NORMED=True)[0]\n",
    "integrate.quad(integrand,0,np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_triexponential_area(RISE=1.0,ETA=2.0,DECAY=5.0,F=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit _triexponential_peak(RISE1=1.0,RISE2=2.0,DECAY=5.0,F=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit optimize.newton(_G0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "\n",
    "def baseline_als(y, lam, p, niter=10):\n",
    "    L = len(y)\n",
    "    D = sparse.diags([1,-2,1],[0,-1,-2], shape=(L,L-2))\n",
    "    w = np.ones(L)\n",
    "    for i in range(niter):\n",
    "        W = sparse.spdiags(w, 0, L, L)\n",
    "        Z = W + lam * D.dot(D.transpose())\n",
    "        z = spsolve(Z, w*y)\n",
    "        w = p * (y > z) + (1-p) * (y < z)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(MEH.t,EA.data[-1])\n",
    "plt.plot(MEH.t,baseline_als(EA.data[-1],10000.,0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highpass_filter(y, sr):\n",
    "    filter_stop_freq = 10  # Hz\n",
    "    filter_pass_freq = 20  # Hz\n",
    "    filter_order = 1001\n",
    "\n",
    "    # High-pass filter\n",
    "    nyquist_rate = sr / 2.\n",
    "    desired = (0, 0, 1, 1)\n",
    "    bands = (0, filter_stop_freq, filter_pass_freq, nyquist_rate)\n",
    "    filter_coefs = signal.firls(filter_order, bands, desired, nyq=nyquist_rate)\n",
    "\n",
    "    # Apply high-pass filter\n",
    "    filtered_audio = signal.filtfilt(filter_coefs, [1], y)\n",
    "    return filtered_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(MEH.t,EA.data[-1])\n",
    "plt.plot(MEH.t,highpass_filter(EA.data[-1],1e3/MEH.dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEH.dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2.7",
   "language": "python",
   "name": "py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
